{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "- 1, 2, 3은 한번만 실행\n",
    "- 4, 5, 6은 반복해서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hypothesis 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = Wx + b\n",
    "W = torch.zeros(1, requires_grad=True)  # requires_grad=True 학습할 것이라 명시\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 학습 (반복 실행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    # 4. hypothesis 예측\n",
    "    hypothesis = x_train * W + b\n",
    "    # 5. cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2) \n",
    "    \n",
    "    # 6. optimizer로 학습 (항상 붙어다니는 3줄)\n",
    "    optimizer.zero_grad()  # gradient 초기화\n",
    "    cost.backward()        # gradient 계산\n",
    "    optimizer.step()       # step()으로 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습과정 확인을 위해 4를 다음과 같이 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch No 100 tensor(0.0004, grad_fn=<MeanBackward0>)\n",
      "epoch No 200 tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "epoch No 300 tensor(0.0002, grad_fn=<MeanBackward0>)\n",
      "epoch No 400 tensor(9.2718e-05, grad_fn=<MeanBackward0>)\n",
      "epoch No 500 tensor(5.7296e-05, grad_fn=<MeanBackward0>)\n",
      "epoch No 600 tensor(3.5404e-05, grad_fn=<MeanBackward0>)\n",
      "epoch No 700 tensor(2.1878e-05, grad_fn=<MeanBackward0>)\n",
      "epoch No 800 tensor(1.3519e-05, grad_fn=<MeanBackward0>)\n",
      "epoch No 900 tensor(8.3542e-06, grad_fn=<MeanBackward0>)\n",
      "epoch No 1000 tensor(5.1626e-06, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "cost_arr = [] \n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    # 4. hypothesis 예측\n",
    "    hypothesis = x_train * W + b\n",
    "    # 5. cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2) \n",
    "    \n",
    "    # 6. optimizer로 학습 (항상 붙어다니는 3줄)\n",
    "    optimizer.zero_grad()  # gradient 초기화\n",
    "    cost.backward()        # gradient 계산\n",
    "    optimizer.step()       # step()으로 개선\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print('epoch No', epoch, cost)\n",
    "    \n",
    "    cost_arr.append(cost.data.numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.9974], requires_grad=True), tensor([0.0060], requires_grad=True))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아주 기본에 충실한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/10 W: 0.0000, Cost: 18.666666\n",
      "Epoch   2/10 W: 2.8000, Cost: 2.986666\n",
      "Epoch   3/10 W: 1.6800, Cost: 0.477867\n",
      "Epoch   4/10 W: 2.1280, Cost: 0.076459\n",
      "Epoch   5/10 W: 1.9488, Cost: 0.012233\n",
      "Epoch   6/10 W: 2.0205, Cost: 0.001957\n",
      "Epoch   7/10 W: 1.9918, Cost: 0.000313\n",
      "Epoch   8/10 W: 2.0033, Cost: 0.000050\n",
      "Epoch   9/10 W: 1.9987, Cost: 0.000008\n",
      "Epoch  10/10 W: 2.0005, Cost: 0.000001\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1)\n",
    "lr = 0.1\n",
    "nb_epochs = 10\n",
    "\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    hypothesis = x_train * W\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    gradient = torch.sum((W * x_train - y_train) * x_train)\n",
    "    \n",
    "    print('Epoch {:3d}/{} W: {:.4f}, Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), cost.item()))\n",
    "    W = W - lr * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('vsc': conda)",
   "language": "python",
   "name": "python37764bitvscconda48cd1de58dbf4dbc9fea30418971662f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
